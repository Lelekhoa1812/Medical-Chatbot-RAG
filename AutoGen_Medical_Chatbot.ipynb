{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpO1du0qRRFi"
      },
      "source": [
        "# Installation of Modules\n",
        "\n",
        "Available GenAI option:\n",
        "- ChatGPT4o: openai\n",
        "- Gemini Flash 2.0: google-genai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Y5-l8MDdRJBx",
        "outputId": "c3a68433-40db-467d-ded4-6e22ebd78c53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.10.0)\n",
            "Requirement already satisfied: autogen-agentchat in /usr/local/lib/python3.11/dist-packages (0.4.5)\n",
            "Requirement already satisfied: autogen-ext in /usr/local/lib/python3.11/dist-packages (0.4.5)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.2.0)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.3.1)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (1.0.1)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.59.9)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: autogen-core==0.4.5 in /usr/local/lib/python3.11/dist-packages (from autogen-agentchat) (0.4.5)\n",
            "Requirement already satisfied: jsonref~=1.1.0 in /usr/local/lib/python3.11/dist-packages (from autogen-core==0.4.5->autogen-agentchat) (1.1.0)\n",
            "Requirement already satisfied: opentelemetry-api>=1.27.0 in /usr/local/lib/python3.11/dist-packages (from autogen-core==0.4.5->autogen-agentchat) (1.30.0)\n",
            "Requirement already satisfied: pillow>=11.0.0 in /usr/local/lib/python3.11/dist-packages (from autogen-core==0.4.5->autogen-agentchat) (11.1.0)\n",
            "Requirement already satisfied: protobuf~=5.29.3 in /usr/local/lib/python3.11/dist-packages (from autogen-core==0.4.5->autogen-agentchat) (5.29.3)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.10.0 in /usr/local/lib/python3.11/dist-packages (from autogen-core==0.4.5->autogen-agentchat) (2.10.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from autogen-core==0.4.5->autogen-agentchat) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.11)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.27.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.47.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.5.1+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.10.0->autogen-core==0.4.5->autogen-agentchat) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.10.0->autogen-core==0.4.5->autogen-agentchat) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.27.0->autogen-core==0.4.5->autogen-agentchat) (1.2.18)\n",
            "Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.27.0->autogen-core==0.4.5->autogen-agentchat) (8.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated>=1.2.6->opentelemetry-api>=1.27.0->autogen-core==0.4.5->autogen-agentchat) (1.17.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api>=1.27.0->autogen-core==0.4.5->autogen-agentchat) (3.21.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install faiss-cpu autogen-agentchat autogen-ext datasets sentence-transformers python-dotenv google-genai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9i92ZgVaVGlW"
      },
      "source": [
        "# Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOjMqOXaVJUf",
        "outputId": "ea9b01cd-8637-49b7-dde0-18ab8b440b43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import drive  # Google Drive support\n",
        "drive.mount('/content/drive')  # Mount Google Drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrueOe0bRrUu"
      },
      "source": [
        "# Inference with OpenAI ChatGPT-4o"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 651
        },
        "id": "V662Cq0ERu1L",
        "outputId": "4079b4af-4724-404a-b2d9-35531ed50c17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Loading medical dataset...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Loaded 10000 medical Q&A pairs.\n",
            "âœ… Generating FAISS vector embeddings...\n",
            "âœ… Loading existing FAISS index...\n",
            "âœ… FAISS index saved successfully!\n",
            "âœ… Initializing RAG-based medical chatbot...\n",
            "âœ… Initializing AI agent with AutoGen...\n",
            "âœ… Medical chatbot is ready!\n",
            "\n",
            "ðŸ©º Medical Chatbot is running...\n",
            "\n",
            "You: \"I've been feeling unusually tired and my vision has become blurry over the past few weeks. I'm also experiencing frequent headaches. Could these be signs of a serious condition?\"\n"
          ]
        },
        {
          "ename": "InvalidRequestError",
          "evalue": "The model `gpt-4` does not exist or you do not have access to it.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-7a5842876b50>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;31m# Prepare JSON reply response body\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;31m# response = chatbot.generate_reply([{\"role\": \"user\", \"content\": user_input}]) # This cannot being used since no Chat classes are defined in autogen module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchatbot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Chatbot:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-7a5842876b50>\u001b[0m in \u001b[0;36mchat\u001b[0;34m(self, user_query)\u001b[0m\n\u001b[1;32m    129\u001b[0m         )\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# 3) Call OpenAI's ChatCompletion (GPT-4)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         completion = openai.ChatCompletion.create(\n\u001b[0m\u001b[1;32m    132\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             messages=[\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/api_resources/chat_completion.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTryAgain\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/api_resources/abstract/engine_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    151\u001b[0m         )\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         response, _, api_key = requestor.request(\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0mrequest_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         )\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpret_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m             return (\n\u001b[0;32m--> 700\u001b[0;31m                 self._interpret_response_line(\n\u001b[0m\u001b[1;32m    701\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mstream_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"error\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream_error\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mrcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m             raise self.handle_error_response(\n\u001b[0m\u001b[1;32m    766\u001b[0m                 \u001b[0mrbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m             )\n",
            "\u001b[0;31mInvalidRequestError\u001b[0m: The model `gpt-4` does not exist or you do not have access to it."
          ]
        }
      ],
      "source": [
        "# ==========================\n",
        "# Medical Chatbot Backend (AutoGen + OpenAI API + RAG)\n",
        "# ==========================\n",
        "\n",
        "# IMPORTANT:\n",
        "# If you see an error like:\n",
        "#   APIRemovedInV1: You tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0\n",
        "#\n",
        "# You have two options:\n",
        "#  1. Migrate your code by running: !openai migrate\n",
        "#  2. Pin your openai version by running: pip install openai==0.28.0\n",
        "#\n",
        "# Choose one before running the script.\n",
        "\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from datasets import load_dataset\n",
        "import faiss\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import autogen_agentchat  # Use autogen_agentchat instead of autogen\n",
        "import autogen_ext  # Use autogen_ext for OpenAI API support\n",
        "import openai\n",
        "import gc # Garbage collection\n",
        "\n",
        "# Define paths for FAISS index and model cache\n",
        "project_dir = \"/content/drive/My Drive/AutoGenRAGMedicalChatbot\"\n",
        "os.makedirs(project_dir, exist_ok=True)  # Ensure directory exists\n",
        "faiss_index_path = os.path.join(project_dir, \"medical_faiss_index\")\n",
        "huggingface_cache_dir = os.path.join(project_dir, \"huggingface_models\")\n",
        "# Set Hugging Face cache directory to avoid re-downloading models\n",
        "os.environ[\"HF_HOME\"] = huggingface_cache_dir\n",
        "\n",
        "\n",
        "# ==========================\n",
        "# Step 1: Load OpenAI API Key\n",
        "# ==========================\n",
        "openai_api_key = \"YOUR_API\"\n",
        "if not openai_api_key:\n",
        "    raise ValueError(\"OpenAI API key is missing!\")\n",
        "openai.api_key = openai_api_key  # Set global OpenAI key\n",
        "\n",
        "\n",
        "# ==========================\n",
        "# Step 2: Load the Medical Dataset from Hugging Face\n",
        "# ==========================\n",
        "print(\"âœ… Loading medical dataset...\")\n",
        "dataset = load_dataset(\"ruslanmv/ai-medical-chatbot\", cache_dir=huggingface_cache_dir)\n",
        "# Extract patient-doctor conversations\n",
        "# medical_dialogues = dataset[\"train\"].to_pandas()[[\"Patient\", \"Doctor\"]]\n",
        "medical_dialogues = dataset[\"train\"].to_pandas()[[\"Patient\", \"Doctor\"]].head(10000)  # Use first 10k/157k rows to reduce RAM usage\n",
        "# Extract patient-doctor conversations\n",
        "print(f\"âœ… Loaded {len(medical_dialogues)} medical Q&A pairs.\")\n",
        "\n",
        "\n",
        "# ==========================\n",
        "# Step 3: Convert Dataset into FAISS Embeddings\n",
        "# ==========================\n",
        "print(\"âœ… Generating FAISS vector embeddings...\")\n",
        "# Load sentence transformer model\n",
        "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=\"cpu\", cache_folder=huggingface_cache_dir)\n",
        "# embedding_model = SentenceTransformer(\"sentence-transformers/paraphrase-MiniLM-L3-v2\", device=\"cpu\", cache_folder=huggingface_cache_dir) # Smaller model for less usage of RAM\n",
        "# Convert text into embeddings\n",
        "medical_qa = [\n",
        "    {\"question\": row[\"Patient\"], \"answer\": row[\"Doctor\"]}\n",
        "    for _, row in medical_dialogues.iterrows()\n",
        "]\n",
        "# Generate vector embeddings\n",
        "medical_embeddings = embedding_model.encode(\n",
        "    [qa[\"question\"] + \" \" + qa[\"answer\"] for qa in medical_qa],\n",
        "    convert_to_numpy=True\n",
        ") # Embedding with float64\n",
        "medical_embeddings = np.array(medical_embeddings, dtype=np.float32) # Re-embedding with float32\n",
        "# Save FAISS index only if it doesnâ€™t exist, this prevents corrupt FAISS indices from causing a crash.\n",
        "if not os.path.exists(faiss_index_path):\n",
        "    print(\"âœ… Creating FAISS index...\")\n",
        "    # Create FAISS index\n",
        "    # index = faiss.IndexFlatL2(medical_embeddings.shape[1])\n",
        "    index = faiss.IndexHNSWFlat(medical_embeddings.shape[1], 32)  # 32 = HNSW graph connections\n",
        "    index.add(medical_embeddings)\n",
        "    faiss.write_index(index, faiss_index_path)\n",
        "    # Manually free up memory\n",
        "    del medical_embeddings\n",
        "    gc.collect()\n",
        "    print(\"âœ… Memory cleared after FAISS indexing.\")\n",
        "else:\n",
        "    print(\"âœ… Loading existing FAISS index...\")\n",
        "print(\"âœ… FAISS index saved successfully!\")\n",
        "\n",
        "\n",
        "# ==========================\n",
        "# Step 4: Retrieval-Augmented Generation (RAG) Implementation\n",
        "# ==========================\n",
        "print(\"âœ… Initializing RAG-based medical chatbot...\")\n",
        "# Load FAISS index for retrieval\n",
        "index = faiss.read_index(faiss_index_path)\n",
        "# Retrieve medical KB using FAISS\n",
        "def retrieve_medical_info(query):\n",
        "    \"\"\"Retrieve relevant medical knowledge using FAISS\"\"\"\n",
        "    query_embedding = embedding_model.encode([query], convert_to_numpy=True)\n",
        "    _, idxs = index.search(query_embedding, k=3)  # Get top 3 matches\n",
        "    return [medical_qa[i][\"answer\"] for i in idxs[0]]\n",
        "\n",
        "\n",
        "# ==========================\n",
        "# Step 5: AutoGen AI Chatbot Implementation (Create new Agent class with autogen_ext)\n",
        "# ==========================\n",
        "print(\"âœ… Initializing AI agent with AutoGen...\")\n",
        "# Init RAG Chatbot custom agent class\n",
        "class RAGMedicalChatbot:\n",
        "    def __init__(self, model_name, retrieve_function):\n",
        "        \"\"\"\n",
        "        A custom retrieval-augmented chatbot using direct OpenAI calls.\n",
        "        :param model_name: e.g., \"gpt-4\"\n",
        "        :param retrieve_function: function to retrieve from FAISS\n",
        "        \"\"\"\n",
        "        self.model_name = model_name\n",
        "        self.retrieve = retrieve_function\n",
        "\n",
        "    def chat(self, user_query):\n",
        "        \"\"\"Generate an answer from GPT-4 using retrieved context.\"\"\"\n",
        "        # 1) Retrieve knowledge\n",
        "        retrieved_info = self.retrieve(user_query)\n",
        "        knowledge_base = \"\\n\".join(retrieved_info)\n",
        "        # 2) Construct final prompt\n",
        "        prompt = (\n",
        "            f\"Using the following medical knowledge:\\n{knowledge_base}\\n\"\n",
        "            f\"Answer the question in a professional and medically accurate manner: {user_query}\"\n",
        "        )\n",
        "        # 3) Call OpenAI's ChatCompletion (GPT-4)\n",
        "        # NOTE: If you see an APIRemovedInV1 error here, please migrate your code or pin the openai package version.\n",
        "        completion = openai.ChatCompletion.create(\n",
        "            model=self.model_name,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful medical chatbot.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            temperature=0.7\n",
        "        )\n",
        "        # 4) Return final response\n",
        "        return completion[\"choices\"][0][\"message\"][\"content\"].strip()\n",
        "\n",
        "# Use OpenAI GPT-4 + RAG setup instantiate the custom chatbot agent\n",
        "chatbot = RAGMedicalChatbot(\n",
        "    model_name=\"gpt-4\",\n",
        "    retrieve_function=retrieve_medical_info\n",
        ")\n",
        "print(\"âœ… Medical chatbot is ready!\")\n",
        "\n",
        "# ==========================\n",
        "# Step 6: Interactive Chat Testing (For Local Debugging)\n",
        "# ==========================\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"\\nðŸ©º Medical Chatbot is running...\\n\")\n",
        "    # Start session\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "        # Type exit or quit to exit the chatbot window ignoring case sensitivit\n",
        "        if user_input.lower() in [\"exit\", \"quit\"]:\n",
        "            print(\"ðŸ‘‹ Chatbot session ended.\")\n",
        "            break\n",
        "        # Prepare JSON reply response body\n",
        "        # response = chatbot.generate_reply([{\"role\": \"user\", \"content\": user_input}]) # This cannot being used since no Chat classes are defined in autogen module\n",
        "        response = chatbot.chat(user_input)\n",
        "        print(\"Chatbot:\", response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECmy5I5irsiH"
      },
      "source": [
        "# Inference with Qwen2.5-Max"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sttDaMq9rxNN"
      },
      "outputs": [],
      "source": [
        "# ==========================\n",
        "# Medical Chatbot Backend (AutoGen + Qwen API + RAG)\n",
        "# ==========================\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from datasets import load_dataset\n",
        "import faiss\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import autogen_agentchat  # Use autogen_agentchat instead of autogen\n",
        "import autogen_ext  # Use autogen_ext for Qwen API support\n",
        "import requests\n",
        "import json\n",
        "import gc  # Garbage collection\n",
        "\n",
        "# Define paths for FAISS index and model cache\n",
        "project_dir = \"/content/drive/My Drive/AutoGenRAGMedicalChatbot\"\n",
        "os.makedirs(project_dir, exist_ok=True)  # Ensure directory exists\n",
        "faiss_index_path = os.path.join(project_dir, \"medical_faiss_index\")\n",
        "huggingface_cache_dir = os.path.join(project_dir, \"huggingface_models\")\n",
        "# Set Hugging Face cache directory to avoid re-downloading models\n",
        "os.environ[\"HF_HOME\"] = huggingface_cache_dir\n",
        "\n",
        "# ==========================\n",
        "# Step 1: Load Qwen API Key\n",
        "# ==========================\n",
        "qwen_api_key = \"your_qwen_api_key_here\"  # Replace with your actual Qwen API key\n",
        "if not qwen_api_key:\n",
        "    raise ValueError(\"Qwen API key is missing!\")\n",
        "\n",
        "# ==========================\n",
        "# Step 2: Load the Medical Dataset from Hugging Face\n",
        "# ==========================\n",
        "print(\"âœ… Loading medical dataset...\")\n",
        "dataset = load_dataset(\"ruslanmv/ai-medical-chatbot\", cache_dir=huggingface_cache_dir)\n",
        "# Extract patient-doctor conversations\n",
        "medical_dialogues = dataset[\"train\"].to_pandas()[[\"Patient\", \"Doctor\"]].head(10000)  # Use first 10k/157k rows to reduce RAM usage\n",
        "print(f\"âœ… Loaded {len(medical_dialogues)} medical Q&A pairs.\")\n",
        "\n",
        "# ==========================\n",
        "# Step 3: Convert Dataset into FAISS Embeddings\n",
        "# ==========================\n",
        "print(\"âœ… Generating FAISS vector embeddings...\")\n",
        "# Load sentence transformer model\n",
        "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=\"cpu\", cache_folder=huggingface_cache_dir)\n",
        "# Convert text into embeddings\n",
        "medical_qa = [\n",
        "    {\"question\": row[\"Patient\"], \"answer\": row[\"Doctor\"]}\n",
        "    for _, row in medical_dialogues.iterrows()\n",
        "]\n",
        "medical_embeddings = embedding_model.encode(\n",
        "    [qa[\"question\"] + \" \" + qa[\"answer\"] for qa in medical_qa],\n",
        "    convert_to_numpy=True\n",
        ")\n",
        "medical_embeddings = np.array(medical_embeddings, dtype=np.float32)\n",
        "\n",
        "# Save FAISS index only if it doesnâ€™t exist\n",
        "if not os.path.exists(faiss_index_path):\n",
        "    print(\"âœ… Creating FAISS index...\")\n",
        "    index = faiss.IndexHNSWFlat(medical_embeddings.shape[1], 32)  # 32 = HNSW graph connections\n",
        "    index.add(medical_embeddings)\n",
        "    faiss.write_index(index, faiss_index_path)\n",
        "    del medical_embeddings\n",
        "    gc.collect()\n",
        "    print(\"âœ… Memory cleared after FAISS indexing.\")\n",
        "else:\n",
        "    print(\"âœ… Loading existing FAISS index...\")\n",
        "\n",
        "print(\"âœ… FAISS index saved successfully!\")\n",
        "\n",
        "# ==========================\n",
        "# Step 4: Retrieval-Augmented Generation (RAG) Implementation\n",
        "# ==========================\n",
        "print(\"âœ… Initializing RAG-based medical chatbot...\")\n",
        "index = faiss.read_index(faiss_index_path)\n",
        "\n",
        "def retrieve_medical_info(query):\n",
        "    \"\"\"Retrieve relevant medical knowledge using FAISS\"\"\"\n",
        "    query_embedding = embedding_model.encode([query], convert_to_numpy=True)\n",
        "    _, idxs = index.search(query_embedding, k=3)  # Get top 3 matches\n",
        "    return [medical_qa[i][\"answer\"] for i in idxs[0]]\n",
        "\n",
        "# ==========================\n",
        "# Step 5: AutoGen AI Chatbot Implementation with Qwen API\n",
        "# ==========================\n",
        "print(\"âœ… Initializing AI agent with AutoGen and Qwen API...\")\n",
        "\n",
        "class QwenMedicalChatbot:\n",
        "    def __init__(self, api_key, retrieve_function):\n",
        "        \"\"\"\n",
        "        A custom retrieval-augmented chatbot using Qwen API.\n",
        "        :param api_key: Qwen API key\n",
        "        :param retrieve_function: function to retrieve from FAISS\n",
        "        \"\"\"\n",
        "        self.api_key = api_key\n",
        "        self.retrieve = retrieve_function\n",
        "\n",
        "    def chat(self, user_query):\n",
        "        \"\"\"Generate an answer from Qwen using retrieved context.\"\"\"\n",
        "        # 1) Retrieve knowledge\n",
        "        retrieved_info = self.retrieve(user_query)\n",
        "        knowledge_base = \"\\n\".join(retrieved_info)\n",
        "\n",
        "        # 2) Construct final prompt\n",
        "        prompt = (\n",
        "            f\"Using the following medical knowledge:\\n{knowledge_base}\\n\"\n",
        "            f\"Answer the question in a professional and medically accurate manner: {user_query}\"\n",
        "        )\n",
        "\n",
        "        # 3) Call Qwen API\n",
        "        url = \"https://api.qwen.com/v1/chat/completions\"  # Replace with the actual Qwen API endpoint\n",
        "        headers = {\n",
        "            \"Authorization\": f\"Bearer {self.api_key}\",\n",
        "            \"Content-Type\": \"application/json\"\n",
        "        }\n",
        "        payload = {\n",
        "            \"model\": \"qwen2.5-max\",\n",
        "            \"messages\": [\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful medical chatbot.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            \"temperature\": 0.7\n",
        "        }\n",
        "\n",
        "        response = requests.post(url, headers=headers, data=json.dumps(payload))\n",
        "        if response.status_code != 200:\n",
        "            raise Exception(f\"Qwen API error: {response.text}\")\n",
        "\n",
        "        result = response.json()\n",
        "        return result[\"choices\"][0][\"message\"][\"content\"].strip()\n",
        "\n",
        "# Instantiate the chatbot with Qwen API\n",
        "chatbot = QwenMedicalChatbot(\n",
        "    api_key=qwen_api_key,\n",
        "    retrieve_function=retrieve_medical_info\n",
        ")\n",
        "print(\"âœ… Medical chatbot is ready!\")\n",
        "\n",
        "# ==========================\n",
        "# Step 6: Interactive Chat Testing (For Local Debugging)\n",
        "# ==========================\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"\\nðŸ©º Medical Chatbot is running...\\n\")\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "        if user_input.lower() in [\"exit\", \"quit\"]:\n",
        "            print(\"ðŸ‘‹ Chatbot session ended.\")\n",
        "            break\n",
        "        response = chatbot.chat(user_input)\n",
        "        print(\"Chatbot:\", response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkrejiDo7KBe"
      },
      "source": [
        "# Inference with Geminish Flash"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6_zO-DO7Jbc",
        "outputId": "43376913-5e05-4c48-ed69-f469534a99b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Loading medical dataset...\n",
            "âœ… Loaded 50000 medical Q&A pairs.\n",
            "âœ… Generating FAISS vector embeddings...\n",
            "âœ… Loading existing FAISS index...\n",
            "âœ… FAISS index saved successfully!\n",
            "âœ… Initializing RAG-based medical chatbot...\n",
            "âœ… Initializing AI agent with custom RAGMedicalChatbot class...\n",
            "âœ… Medical chatbot is ready!\n",
            "\n",
            "ðŸ©º Medical Chatbot is running...\n",
            "\n",
            "You: I have a headache and feeling dizzy, what illness is this?\n",
            "Chatbot: It is impossible to diagnose the cause of your headache and dizziness without a thorough medical evaluation. Based on the medical knowledge provided, several possibilities exist:\n",
            "\n",
            "*   **Blood Pressure Issues:** Fluctuations or high blood pressure can cause headaches and dizziness.\n",
            "*   **Otological Causes:** An ear infection could be responsible.\n",
            "*   **Viral Respiratory Tract Infection:** If you have congestion, a viral infection might be the cause.\n",
            "*   **Lifestyle Factors:** Stress and smoking habits can contribute to headaches.\n",
            "*   **Cardiac Issues:** Reduced blood flow to the brain due to a heart condition can cause dizziness.\n",
            "*   **Brain Lesion:** A brain lesion may lead to these symptoms. An MRI is recommended.\n",
            "*   **Stroke-like Condition:** If slurred speech is present, along with hypertension and no other identified cause, a CT scan might be necessary to rule out a stroke.\n",
            "\n",
            "**Recommendation:**\n",
            "\n",
            "Given the variety of potential causes, it is crucial to **consult a physician** for a comprehensive physical examination and appropriate investigations. They can assess your specific symptoms, medical history, and conduct necessary tests to determine the underlying cause and recommend the appropriate treatment plan.\n",
            "Response time: 2.52 seconds\n"
          ]
        }
      ],
      "source": [
        "# ==========================\n",
        "# Medical Chatbot Backend (AutoGen + Gemini Flash API + RAG)\n",
        "# ==========================\n",
        "\n",
        "# IMPORTANT:\n",
        "# This script utilise the Gemini Flash API. Model: gemini-2.0-flash\n",
        "# Ensure you have the correct Gemini Flash API endpoint and payload format.\n",
        "\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from datasets import load_dataset\n",
        "import faiss\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import autogen_agentchat # (Not used for agent classes in this version)\n",
        "import autogen_ext       # (Not used for agent creation in this version)\n",
        "import requests          # To make HTTP requests to Gemini Flash API\n",
        "from google import genai # Use Gemini Flash GenAI model\n",
        "import gc                # Garbage collection\n",
        "import time\n",
        "\n",
        "# --------------------------\n",
        "# Set up paths and cache directories on Google Drive\n",
        "# --------------------------\n",
        "project_dir = \"/content/drive/My Drive/AutoGenRAGMedicalChatbot\"\n",
        "os.makedirs(project_dir, exist_ok=True)  # Ensure directory exists\n",
        "faiss_index_path = os.path.join(project_dir, \"medical_faiss_index\")\n",
        "huggingface_cache_dir = os.path.join(project_dir, \"huggingface_models\")\n",
        "# Set Hugging Face cache directory to avoid re-downloading models\n",
        "os.environ[\"HF_HOME\"] = huggingface_cache_dir\n",
        "\n",
        "# --------------------------\n",
        "# Step 1: Load Gemini Flash API Key\n",
        "# --------------------------\n",
        "gemini_flash_api_key = \"YOUR_API\"\n",
        "if not gemini_flash_api_key:\n",
        "    raise ValueError(\"Gemini Flash API key is missing!\")\n",
        "\n",
        "# --------------------------\n",
        "# Step 2: Load the Medical Dataset from Hugging Face\n",
        "# --------------------------\n",
        "print(\"âœ… Loading medical dataset...\")\n",
        "dataset = load_dataset(\"ruslanmv/ai-medical-chatbot\", cache_dir=huggingface_cache_dir)\n",
        "# For reduced RAM usage, we use the first 50k/157k rows.\n",
        "medical_dialogues = dataset[\"train\"].to_pandas()[[\"Patient\", \"Doctor\"]].head(50000)\n",
        "print(f\"âœ… Loaded {len(medical_dialogues)} medical Q&A pairs.\")\n",
        "\n",
        "# --------------------------\n",
        "# Step 3: Convert Dataset into FAISS Embeddings\n",
        "# --------------------------\n",
        "print(\"âœ… Generating FAISS vector embeddings...\")\n",
        "# Load SentenceTransformer model on CPU\n",
        "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=\"cpu\", cache_folder=huggingface_cache_dir)\n",
        "# Prepare Q&A data\n",
        "medical_qa = [\n",
        "    {\"question\": row[\"Patient\"], \"answer\": row[\"Doctor\"]}\n",
        "    for _, row in medical_dialogues.iterrows()\n",
        "]\n",
        "# Generate vector embeddings and convert to float32 for FAISS\n",
        "medical_embeddings = embedding_model.encode(\n",
        "    [qa[\"question\"] + \" \" + qa[\"answer\"] for qa in medical_qa],\n",
        "    convert_to_numpy=True\n",
        ").astype(np.float32)\n",
        "\n",
        "# Create or load the FAISS index\n",
        "if not os.path.exists(faiss_index_path):\n",
        "    print(\"âœ… Creating FAISS index...\")\n",
        "    index = faiss.IndexHNSWFlat(medical_embeddings.shape[1], 32)  # 32 = HNSW graph connections\n",
        "    index.add(medical_embeddings)\n",
        "    faiss.write_index(index, faiss_index_path)\n",
        "    del medical_embeddings\n",
        "    gc.collect()\n",
        "    print(\"âœ… Memory cleared after FAISS indexing.\")\n",
        "else:\n",
        "    print(\"âœ… Loading existing FAISS index...\")\n",
        "    index = faiss.read_index(faiss_index_path)\n",
        "print(\"âœ… FAISS index saved successfully!\")\n",
        "\n",
        "# --------------------------\n",
        "# Step 4: Retrieval-Augmented Generation (RAG) Implementation\n",
        "# --------------------------\n",
        "print(\"âœ… Initializing RAG-based medical chatbot...\")\n",
        "def retrieve_medical_info(query):\n",
        "    \"\"\"Retrieve relevant medical knowledge using FAISS\"\"\"\n",
        "    query_embedding = embedding_model.encode([query], convert_to_numpy=True)\n",
        "    _, idxs = index.search(query_embedding, k=3)  # Get top 3 matches\n",
        "    return [medical_qa[i][\"answer\"] for i in idxs[0]]\n",
        "\n",
        "# --------------------------\n",
        "# Step 5: Custom RAG Medical Chatbot Agent Class using Gemini Flash API\n",
        "# --------------------------\n",
        "print(\"âœ… Initializing AI agent with custom RAGMedicalChatbot class...\")\n",
        "\n",
        "def gemini_flash_completion(prompt, model, temperature=0.7):\n",
        "    \"\"\"\n",
        "    Call the Gemini Flash API to get a completion.\n",
        "    Adjust the endpoint and payload as needed based on the API specification.\n",
        "    \"\"\"\n",
        "    # # Construct the endpoint URL using the provided model and API key\n",
        "    # endpoint = f\"https://generativelanguage.googleapis.com/v1beta/models/{model}:generateContent?key={gemini_flash_api_key}\"\n",
        "    # headers = {\n",
        "    #         \"Content-Type\": \"application/json\"\n",
        "    #     }\n",
        "    # # Format the payload as per the test CURL example\n",
        "    # payload = {\n",
        "    #     \"contents\": [{\n",
        "    #         \"parts\": [{\"text\": prompt}]\n",
        "    #     }]\n",
        "    # }\n",
        "    # response = requests.post(endpoint, headers=headers, json=payload)\n",
        "    # response.raise_for_status()  # Raise an error for bad responses\n",
        "    # data = response.json()\n",
        "    # return data[\"candidates\"][0][\"output\"][\"text\"]\n",
        "    client = genai.Client(api_key=gemini_flash_api_key)\n",
        "    try:\n",
        "        response = client.models.generate_content(\n",
        "            model=model, contents=prompt\n",
        "        )\n",
        "        return response.text  # Extract the text from the response\n",
        "    except Exception as e:\n",
        "        print(f\"Error calling Gemini API: {e}\")\n",
        "        return None  # Or handle the error as needed\n",
        "\n",
        "class RAGMedicalChatbot:\n",
        "    def __init__(self, model_name, retrieve_function):\n",
        "        \"\"\"\n",
        "        A custom retrieval-augmented chatbot using Gemini Flash API.\n",
        "        :param model_name: e.g., \"gemini-2.0-flash\" (adjust based on the actual model name)\n",
        "        :param retrieve_function: function to retrieve context from FAISS\n",
        "        \"\"\"\n",
        "        self.model_name = model_name\n",
        "        self.retrieve = retrieve_function\n",
        "\n",
        "    def chat(self, user_query):\n",
        "        \"\"\"Generate an answer from Gemini Flash API using retrieved context.\"\"\"\n",
        "        # 1) Retrieve relevant knowledge\n",
        "        retrieved_info = self.retrieve(user_query)\n",
        "        knowledge_base = \"\\n\".join(retrieved_info)\n",
        "        # 2) Construct final prompt\n",
        "        prompt = (\n",
        "            f\"Using the following medical knowledge:\\n{knowledge_base}\\n\"\n",
        "            f\"Answer the question in a professional and medically accurate manner: {user_query}\"\n",
        "        )\n",
        "        # 3) Call Gemini Flash API for completion\n",
        "        completion = gemini_flash_completion(prompt, model=self.model_name, temperature=0.7)\n",
        "        # 4) Return the final response\n",
        "        return completion.strip()\n",
        "\n",
        "# Instantiate the custom chatbot agent.\n",
        "# Replace \"gemini-2.0-flash\" with the actual model identifier if needed.\n",
        "chatbot = RAGMedicalChatbot(\n",
        "    model_name=\"gemini-2.0-flash\",\n",
        "    retrieve_function=retrieve_medical_info\n",
        ")\n",
        "print(\"âœ… Medical chatbot is ready!\")\n",
        "\n",
        "# --------------------------\n",
        "# Step 6: Interactive Chat Testing (For Local Debugging)\n",
        "# --------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"\\nðŸ©º Medical Chatbot is running...\\n\")\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "        if user_input.lower() in [\"exit\", \"quit\"]:\n",
        "            print(\"ðŸ‘‹ Chatbot session ended.\")\n",
        "            break\n",
        "        start_time = time.time()  # Start timing\n",
        "        response = chatbot.chat(user_input)\n",
        "        end_time = time.time()  # End timing\n",
        "        if response: #Check if response exists before printing\n",
        "            print(\"Chatbot:\", response)\n",
        "            print(f\"Response time: {end_time - start_time:.2f} seconds\") # Print response time\n",
        "        else:\n",
        "            print(\"Error generating response.\") # Print error message\n",
        "        gc.collect() # Collect garbage after each loop for memory management"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "HpO1du0qRRFi",
        "QrueOe0bRrUu",
        "ECmy5I5irsiH"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.7 (main, Oct 10 2024, 10:50:01) [Clang 14.0.0 (clang-1400.0.29.202)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "f1062708a37074d70712b695aadee582e0b0b9f95f45576b5521424137d05fec"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
