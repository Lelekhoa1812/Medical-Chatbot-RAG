## **AutoGen-Based Medical Chatbot with RAG**

This guide will help you deploy a **Medical Chatbot** using **AutoGen**, **RAG (Retrieval-Augmented Generation)**, and **OpenAI API** with a **custom Hugging Face medical dataset**.

---

## **Installation**
### **Step 1: Set Up Conda Environment**
Create a new Conda environment with Python 3.11:   
```bash
conda create -n chatbot_env python=3.11 -y
conda activate chatbot_env
```
**Notice:** Ensure your machine has required Anaconda or Miniconda version installed.  

---

### **Step 2: Install Dependencies**
Ensure your environment has the necessary libraries:   
```bash
pip install -U "autogen-agentchat" "autogenstudio" \
                "faiss-cpu" "chromadb" "datasets" "sentence-transformers" "python-dotenv" "google-genai" "huggingface_hub" "pymongo" \
                "uvicorn" "fastapi"
```
Or install all:  
```bash
pip install -U requirements.txt
```

- `faiss-cpu`: Efficient vector search for RAG.
- `chromadb`: Local vector database for retrieval (Not using now).
- `datasets`: Loading and processing custom data.
- `sentence-transformers`: Converting text into embeddings.
- `python-dotenv`: Managing API keys securely from `.env`.
- `pymongo`: Utilization of MongoDB and gridfs (for compressed FAISS data saving).

Installation of server and router on FastAPI for a client-server interface (optional):  
```bash
pip install fastapi uvicorn requests python-multipart
```

---

## **Configuration**
### **Step 3: Set Up a genAI API Key**
To integrate RAG and answer intelligently, you'll need an **OpenAI API key**:  
```bash
export OPENAI_API_KEY="your_openai_api_key"
```
Alternatively using Flash GeminiAPI for lower/free-tier cost:
```bash
export Flash_API="your_gemini_api_key"
```

Alternatively, store it in a `.env` file:
```bash
echo 'OPENAI_API_KEY="your_openai_api_key"' > .env
```
Or:
```bash
echo 'Flash_API="your_gemini_api_key"' > .env
```

### **Optional: Saving embedded vector database to MongoDB**
Store MongoDB cluster API key into `.env` file. You need 2 MongoDB clusters (you can store in 1 individually, however, notice that it could consume high storage availability and wouldn't be ideal for free tier).

1. First cluster (db) storing your QA entries data:
```bash
echo 'MONGO_URI=your_mongodb1_api_key' > .env
```

2. Second cluster (db) storing your embedded FAISS vector index:
```bash
echo 'INDEX_URI=your_mongodb2_api_key' > .env
```

---

## **Project Structure**
```plaintext
/Medical-Chatbot         # Backend
  â”œâ”€â”€ app.py             # main server
  â”œâ”€â”€ download_model.py  # model loader
  â”œâ”€â”€ clear_mongo.py     # debugs
  â”œâ”€â”€ connect_mongo.py
  â”œâ”€â”€ migrate.py 
  â”œâ”€â”€ requirements.txt   # module installation
  â”œâ”€â”€ Dockerfile         # configs and build
  â”œâ”€â”€ .env
  â”œâ”€â”€ .huggingface.yml
/static                  # Frontend
  â”œâ”€â”€ dist               # built-in package by vite
  â”œâ”€â”€ node_modules
  â”œâ”€â”€ index.html         # interface level
  â”œâ”€â”€ script.js
  â”œâ”€â”€ styles.css
  â”œâ”€â”€ img
  â”œâ”€â”€ vercel.json        # build configs
  â”œâ”€â”€ vite.config.js
  â”œâ”€â”€ package.json
  â”œâ”€â”€ package-lock.json
â”œâ”€â”€ setup.md
â”œâ”€â”€ README.md
â”œâ”€â”€ Embedding.ipynb
```

## **LLM Model Caching**
This is the directory and list of nested files for LLM model `MiniLM-L6-v2`, which serve as the backbone for Docbot.
```bash
ğŸ“ /app/model_cache/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/fa97f6e7cb1a59073dff9e6b13e2715cf7475ac9/
  ğŸ“„ README.md
  ğŸ“„ config_sentence_transformers.json
  ğŸ“„ config.json
  ğŸ“„ modules.json
  ğŸ“„ data_config.json
  ğŸ“„ .gitattributes
  ğŸ“„ sentence_bert_config.json
  ğŸ“„ special_tokens_map.json
  ğŸ“„ tokenizer.json
  ğŸ“„ tokenizer_config.json
  ğŸ“„ model.safetensors
  ğŸ“„ train_script.py
  ğŸ“„ vocab.txt
  ğŸ“„ pytorch_model.bin
  ğŸ“„ rust_model.ot
  ğŸ“„ tf_model.h5
```